{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "0 http://feeds.feedburner.com/37signals/beMH\n",
      "\n",
      "1 http://feeds.feedburner.com/blogspot/bRuz\n",
      "\n",
      "2 http://battellemedia.com/index.xml\n",
      "\n",
      "3 http://blog.guykawasaki.com/index.rdf\n",
      "\n",
      "4 http://blog.outer-court.com/rss.xml\n",
      "\n",
      "5 http://feeds.searchenginewatch.com/sewblog\n",
      "\n",
      "6 http://blog.topix.net/index.rdf\n",
      "\n",
      "7 http://blogs.abcnews.com/theblotter/index.rdf\n",
      "\n",
      "8 http://feeds.feedburner.com/ConsumingExperienceFull\n",
      "\n",
      "9 http://flagrantdisregard.com/index.php/feed/\n",
      "\n",
      "10 http://featured.gigaom.com/feed/\n",
      "\n",
      "11 http://gizmodo.com/index.xml\n",
      "\n",
      "12 http://gofugyourself.typepad.com/go_fug_yourself/index.rdf\n",
      "\n",
      "13 http://googleblog.blogspot.com/rss.xml\n",
      "\n",
      "14 http://feeds.feedburner.com/GoogleOperatingSystem\n",
      "\n",
      "15 http://headrush.typepad.com/creating_passionate_users/index.rdf\n",
      "\n",
      "16 http://feeds.feedburner.com/instapundit/main\n",
      "\n",
      "17 http://jeremy.zawodny.com/blog/rss2.xml\n",
      "\n",
      "18 http://joi.ito.com/index.rdf\n",
      "\n",
      "19 http://feeds.feedburner.com/Mashable\n",
      "\n",
      "20 http://michellemalkin.com/index.rdf\n",
      "\n",
      "21 http://moblogsmoproblems.blogspot.com/rss.xml\n",
      "\n",
      "22 http://newsbusters.org/node/feed\n",
      "\n",
      "23 http://beta.blogger.com/feeds/27154654/posts/full?alt=rss\n",
      "\n",
      "24 http://feeds.feedburner.com/paulstamatiou\n",
      "\n",
      "25 http://powerlineblog.com/index.rdf\n",
      "\n",
      "26 http://feeds.feedburner.com/Publishing20\n",
      "\n",
      "27 http://radar.oreilly.com/index.rdf\n",
      "\n",
      "28 http://scienceblogs.com/pharyngula/index.xml\n",
      "\n",
      "29 http://scobleizer.wordpress.com/feed/\n",
      "\n",
      "30 http://sethgodin.typepad.com/seths_blog/index.rdf\n",
      "\n",
      "31 http://rss.slashdot.org/Slashdot/slashdot\n",
      "\n",
      "32 http://thinkprogress.org/feed/\n",
      "\n",
      "33 http://feeds.feedburner.com/andrewsullivan/rApM\n",
      "\n",
      "34 http://wilwheaton.typepad.com/wwdnbackup/index.rdf\n",
      "\n",
      "35 http://www.43folders.com/feed/\n",
      "\n",
      "36 http://www.456bereastreet.com/feed.xml\n",
      "\n",
      "37 http://www.autoblog.com/rss.xml\n",
      "\n",
      "38 http://www.bloggersblog.com/rss.xml\n",
      "\n",
      "39 http://www.bloglines.com/rss/about/news\n",
      "\n",
      "40 http://www.blogmaverick.com/rss.xml\n",
      "\n",
      "41 http://www.boingboing.net/index.rdf\n",
      "\n",
      "42 http://www.buzzmachine.com/index.xml\n",
      "\n",
      "43 http://www.captainsquartersblog.com/mt/index.rdf\n",
      "\n",
      "44 http://www.coolhunting.com/index.rdf\n",
      "\n",
      "45 http://feeds.copyblogger.com/Copyblogger\n",
      "\n",
      "46 http://feeds.feedburner.com/crooksandliars/YaCP\n",
      "\n",
      "47 http://feeds.dailykos.com/dailykos/index.xml\n",
      "\n",
      "48 http://www.deadspin.com/index.xml\n",
      "\n",
      "49 http://www.downloadsquad.com/rss.xml\n",
      "\n",
      "50 http://www.engadget.com/rss.xml\n",
      "\n",
      "51 http://www.gapingvoid.com/index.rdf\n",
      "\n",
      "52 http://www.gawker.com/index.xml\n",
      "\n",
      "53 http://www.gothamist.com/index.rdf\n",
      "\n",
      "54 http://www.huffingtonpost.com/raw_feed_index.rdf\n",
      "\n",
      "55 http://www.hyperorg.com/blogger/index.rdf\n",
      "\n",
      "56 http://www.joelonsoftware.com/rss.xml\n",
      "\n",
      "57 http://www.joystiq.com/rss.xml\n",
      "\n",
      "58 http://www.kotaku.com/index.xml\n",
      "\n",
      "59 http://feeds.kottke.org/main\n",
      "\n",
      "60 http://www.lifehack.org/feed/\n",
      "\n",
      "61 http://www.lifehacker.com/index.xml\n",
      "\n",
      "62 http://littlegreenfootballs.com/weblog/lgf-rss.php\n",
      "\n",
      "63 http://www.makezine.com/blog/index.xml\n",
      "\n",
      "64 http://www.mattcutts.com/blog/feed/\n",
      "\n",
      "65 http://xml.metafilter.com/rss.xml\n",
      "\n",
      "66 http://www.mezzoblue.com/rss/index.xml\n",
      "\n",
      "67 http://www.micropersuasion.com/index.rdf\n",
      "\n",
      "68 http://www.neilgaiman.com/journal/feed/rss.xml\n",
      "\n",
      "69 http://www.oilman.ca/feed/\n",
      "\n",
      "70 http://www.perezhilton.com/index.xml\n",
      "\n",
      "71 http://www.plasticbag.org/index.rdf\n",
      "\n",
      "72 http://www.powazek.com/rss.xml\n",
      "\n",
      "73 http://www.problogger.net/feed/\n",
      "\n",
      "74 http://feeds.feedburner.com/QuickOnlineTips\n",
      "\n",
      "75 http://www.readwriteweb.com/rss.xml\n",
      "\n",
      "76 http://www.schneier.com/blog/index.rdf\n",
      "\n",
      "77 http://scienceblogs.com/sample/combined.xml\n",
      "\n",
      "78 http://www.seroundtable.com/index.rdf\n",
      "\n",
      "79 http://www.shoemoney.com/feed/\n",
      "\n",
      "80 http://www.sifry.com/alerts/index.rdf\n",
      "\n",
      "81 http://www.simplebits.com/xml/rss.xml\n",
      "\n",
      "82 http://feeds.feedburner.com/Spikedhumor\n",
      "\n",
      "83 http://www.stevepavlina.com/blog/feed\n",
      "\n",
      "84 http://www.talkingpointsmemo.com/index.xml\n",
      "\n",
      "85 http://www.tbray.org/ongoing/ongoing.rss\n",
      "\n",
      "86 http://feeds.feedburner.com/TechCrunch\n",
      "\n",
      "87 http://www.techdirt.com/techdirt_rss.xml\n",
      "\n",
      "88 http://www.techeblog.com/index.php/feed/\n",
      "\n",
      "89 http://www.thesuperficial.com/index.xml\n",
      "\n",
      "90 http://www.tmz.com/rss.xml\n",
      "\n",
      "91 http://www.treehugger.com/index.rdf\n",
      "\n",
      "92 http://www.tuaw.com/rss.xml\n",
      "\n",
      "93 http://www.valleywag.com/index.xml\n",
      "\n",
      "94 http://www.we-make-money-not-art.com/index.rdf\n",
      "\n",
      "95 http://www.wired.com/rss/index.xml\n",
      "\n",
      "96 http://www.wonkette.com/index.xml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# counting the words in a feed\n",
    "import feedparser\n",
    "import re\n",
    "\n",
    "def getwordcounts(url):\n",
    "    d = feedparser.parse(url)\n",
    "    wc = {}\n",
    "    \n",
    "    for e in d.entries:\n",
    "        if 'summary' in e: summary = e.summary\n",
    "        else: summary = e.description\n",
    "    \n",
    "        words = getwords(e.title+' '+summary)\n",
    "        for word in words:\n",
    "            wc.setdefault(word, 0)\n",
    "            wc[word] += 1\n",
    "    return d.feed.title, wc\n",
    "\n",
    "def getwords(html):\n",
    "    txt = re.compile(r'<[^>]+>').sub('', html)\n",
    "    \n",
    "    words = re.compile(r'[^A-Z^a-z]+').split(txt)\n",
    "    \n",
    "    return [word.lower() for word in words if word != '']\n",
    "\n",
    "apcount = {}\n",
    "wordcounts = {}\n",
    "feedlist = [line for line in open('feedlist.txt')]\n",
    "for i, feedurl in enumerate(feedlist):\n",
    "    print(i, feedurl)\n",
    "    try:\n",
    "        title, wc = getwordcounts(feedurl)\n",
    "    except:\n",
    "        continue\n",
    "    wordcounts[title] = wc\n",
    "    for word, count in wc.items():\n",
    "        apcount.setdefault(word, 0)\n",
    "        if count>1:\n",
    "            apcount[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "for w, bc in apcount.items():\n",
    "    frac = float(bc) / len(feedlist)\n",
    "    if frac > 0.1 and frac < 0.5:\n",
    "        wordlist.append(w)\n",
    "\n",
    "with open('blogdata.txt', 'w') as out:\n",
    "    out.write('Blog')\n",
    "    for word in wordlist:\n",
    "        out.write('\\t{0}'.format(word))\n",
    "    out.write('\\n')\n",
    "    for blog, wc in wordcounts.items():\n",
    "        out.write(blog)\n",
    "        for word in wordlist:\n",
    "            if word in wc: out.write('\\t{0}'.format(wc[word]))\n",
    "            else: out.write('\\t0')\n",
    "        out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(wordcounts)\n",
    "df = df.loc[df.index.isin(wordlist)]\n",
    "df.to_csv('blogdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = [line for line in f]\n",
    "    \n",
    "    colnames = lines[0].strip().split('\\t')[1:]\n",
    "    rownames = []\n",
    "    data = []\n",
    "    for line in lines[1:]:\n",
    "        p = line.strip().split('\\t')\n",
    "        rownames.append(p[0])\n",
    "        data.append([float(x) for x in p[1:]])\n",
    "    \n",
    "    return rownames, colnames, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def pearson(v1, v2):\n",
    "    sum1 = sum(v1)\n",
    "    sum2 = sum(v2)\n",
    "    \n",
    "    sum1Sq = sum([pow(v, 2) for v in v1])\n",
    "    sum2Sq = sum([pow(v, 2) for v in v2])\n",
    "    \n",
    "    pSum = sum([v1[i] * v2[i] for i in range(len(v1))])\n",
    "    \n",
    "    num = pSum - (sum1 * sum2 / len(v1))\n",
    "    den = sqrt((sum1Sq - pow(sum1, 2)/len(v1)) * (sum1Sq - pow(sum1, 2)/len(v1)))\n",
    "    \n",
    "    if den == 0: return 0\n",
    "    \n",
    "    return 1.0 - num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bicluster:\n",
    "    def __init__(self, vec, left=None, right=None,\n",
    "                 distance=0.0, id=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.vec = vec\n",
    "        self.id = id\n",
    "        self.distance = distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hcluster(rows, distance=pearson):\n",
    "    distances = {}\n",
    "    currentclustid = -1\n",
    "    \n",
    "    clust = [bicluster(rows[i], id=i)\n",
    "             for i in range(len(rows))]\n",
    "    \n",
    "    while len(clust) > 1:\n",
    "        lowestpair = (0, 1)\n",
    "        clostest = distance(clust[0].vec, clust[1].vec)\n",
    "        \n",
    "        for i in range(len(clust)):\n",
    "            for j in range(i+1, len(clust)):\n",
    "                if (clust[i].id, clust[j].id) not in distances:\n",
    "                    distances[(clust[i].id, clust[j].id)] = \\\n",
    "                        distance(clust[i].vec, clust[j].vec)\n",
    "                    \n",
    "                d = distances[(clust[i].id, clust[j].id)]\n",
    "                \n",
    "                if d < clostest:\n",
    "                    clostest = d\n",
    "                    lowestpair = (i, j)\n",
    "        \n",
    "        mergevec = [\n",
    "            (clust[lowestpair[0]].vec[i] + clust[lowestpair[1]].vec[i])/2.0\n",
    "            for i in range(len(clust[0].vec))\n",
    "        ]\n",
    "        \n",
    "        newcluster = bicluster(mergevec,\n",
    "                               left=clust[lowestpair[0]],\n",
    "                               right=clust[lowestpair[1]],\n",
    "                               distance=clostest,\n",
    "                               id=currentclustid\n",
    "                              )\n",
    "        currentclustid -= 1\n",
    "        del clust[lowestpair[1]]\n",
    "        del clust[lowestpair[0]]\n",
    "        clust.append(newcluster)\n",
    "        \n",
    "    return clust[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blognames, words, data = readfile('blogdata.txt')\n",
    "clust = hcluster(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printclust(clust, labels=None, n=0):\n",
    "    \n",
    "    for i in range(n): print(' ', end='')\n",
    "    if clust.id < 0:\n",
    "        print('-')\n",
    "    else:\n",
    "        if labels == None: print(clust.id)\n",
    "        else: print(labels[clust.id])\n",
    "    \n",
    "    if clust.left != None:\n",
    "        printclust(clust.left, labels=labels, n=n+1)\n",
    "    if clust.right != None:\n",
    "        printclust(clust.right, labels=labels, n=n+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      " Kotaku\n",
      " -\n",
      "  Schneier on Security\n",
      "  -\n",
      "   Derek Powazek\n",
      "   -\n",
      "    gapingvoid: \"cartoons drawn on the back of business cards\"\n",
      "    -\n",
      "     -\n",
      "      -\n",
      "       -\n",
      "        -\n",
      "         Power Line\n",
      "         NewsBusters.org - Exposing Liberal Media Bias\n",
      "        -\n",
      "         Think Progress\n",
      "         -\n",
      "          Talking Points Memo: by Joshua Micah Marshall\n",
      "          Daily Kos\n",
      "       -\n",
      "        Hot Air\n",
      "        SpikedHumor\n",
      "      -\n",
      "       -\n",
      "        PerezHilton.com\n",
      "        Jeremy Zawodny's blog\n",
      "       -\n",
      "        -\n",
      "         -\n",
      "          -\n",
      "           -\n",
      "            -\n",
      "             -\n",
      "              -\n",
      "               -\n",
      "                -\n",
      "                 Wired News: Top Stories\n",
      "                 -\n",
      "                  GigaOM\n",
      "                  Lifehacker\n",
      "                -\n",
      "                 -\n",
      "                  TechCrunch\n",
      "                  -\n",
      "                   Matt Cutts: Gadgets, Google, and SEO\n",
      "                   Read/WriteWeb\n",
      "                 -\n",
      "                  -\n",
      "                   Shoemoney - Skills to pay the bills\n",
      "                   -\n",
      "                    Search Engine Roundtable\n",
      "                    -\n",
      "                     John Battelle's Searchblog\n",
      "                     Google Operating System\n",
      "                  -\n",
      "                   Google Blogoscoped\n",
      "                   -\n",
      "                    Search Engine Watch Blog\n",
      "                    Official Google Blog\n",
      "               -\n",
      "                Scobleizer - Tech Geek Blogger\n",
      "                -\n",
      "                 Mashable!\n",
      "                 Valleywag\n",
      "              -\n",
      "               -\n",
      "                ProBlogger Blog Tips\n",
      "                -\n",
      "                 -\n",
      "                  Oilman\n",
      "                  -\n",
      "                   BuzzMachine\n",
      "                   Sifry's Alerts\n",
      "                 -\n",
      "                  lifehack.org\n",
      "                  The Viral Garden\n",
      "               -\n",
      "                Topix.net Weblog\n",
      "                Bloggers Blog: Blogging the Blogsphere\n",
      "             -\n",
      "              -\n",
      "               -\n",
      "                MetaFilter\n",
      "                -\n",
      "                 -\n",
      "                  Captain's Quarters\n",
      "                  -\n",
      "                   Michelle Malkin\n",
      "                   Gothamist\n",
      "                 -\n",
      "                  -\n",
      "                   -\n",
      "                    The Blotter\n",
      "                    -\n",
      "                     Andrew Sullivan | The Daily Dish\n",
      "                     Little Green Footballs\n",
      "                   -\n",
      "                    -\n",
      "                     Crooks and Liars\n",
      "                     -\n",
      "                      Instapundit.com\n",
      "                      The Huffington Post | Raw Feed\n",
      "                    -\n",
      "                     -\n",
      "                      flagrantdisregard\n",
      "                      -\n",
      "                       The Superficial - Because You're Ugly\n",
      "                       Wonkette\n",
      "                     -\n",
      "                      Neil Gaiman's Journal\n",
      "                      Deadspin\n",
      "                  -\n",
      "                   Eschaton\n",
      "                   we make money not art\n",
      "               -\n",
      "                Joi Ito's Web\n",
      "                -\n",
      "                 Boing Boing\n",
      "                 -\n",
      "                  Slashdot\n",
      "                  MAKE Magazine\n",
      "              -\n",
      "               Treehugger\n",
      "               kottke.org\n",
      "            -\n",
      "             -\n",
      "              -\n",
      "               Creating Passionate Users\n",
      "               -\n",
      "                Joho the Blog\n",
      "                456 Berea Street\n",
      "              -\n",
      "               -\n",
      "                Steve Pavlina's Personal Development Blog\n",
      "                -\n",
      "                 SimpleBits\n",
      "                 Cool Hunting\n",
      "               -\n",
      "                Pharyngula\n",
      "                -\n",
      "                 Online Marketing Report\n",
      "                 ScienceBlogs : Combined Feed\n",
      "             -\n",
      "              -\n",
      "               WWdN: In Exile\n",
      "               -\n",
      "                Gizmodo\n",
      "                Engadget\n",
      "              -\n",
      "               -\n",
      "                Signum sine tinnitu--by Guy Kawasaki\n",
      "                Joel on Software\n",
      "               -\n",
      "                -\n",
      "                 -\n",
      "                  Seth's Blog\n",
      "                  -\n",
      "                   Publishing 2.0\n",
      "                   Blog Maverick\n",
      "                 -\n",
      "                  CoolerHeads Prevail\n",
      "                  The Unofficial Apple Weblog (TUAW)\n",
      "                -\n",
      "                 Joystiq\n",
      "                 Download Squad\n",
      "           -\n",
      "            ongoing\n",
      "            -\n",
      "             Go Fug Yourself\n",
      "             Gawker\n",
      "          -\n",
      "           -\n",
      "            43 Folders\n",
      "            PaulStamatiou.com\n",
      "           -\n",
      "            O'Reilly Radar\n",
      "            Bloglines | News\n",
      "         -\n",
      "          -\n",
      "           Copyblogger\n",
      "           -\n",
      "            Micro Persuasion\n",
      "            Quick Online Tips\n",
      "          -\n",
      "           TMZ.com\n",
      "           Autoblog\n",
      "        -\n",
      "         -\n",
      "          Dave Shea's mezzoblue\n",
      "          plasticbag.org\n",
      "         -\n",
      "          Signal vs. Noise\n",
      "          Techdirt\n",
      "     -\n",
      "      TechEBlog\n",
      "      A Consuming Experience (full feed)\n"
     ]
    }
   ],
   "source": [
    "printclust(clust, blognames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drawing the Dendrogram\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getheight(clust):\n",
    "    if clust.left == None and clust.right == None:\n",
    "        return 1\n",
    "    return getheight(clust.right) + getheight(clust.left)\n",
    "\n",
    "def getdepth(clust):\n",
    "    if clust.left == None and clust.right == None:\n",
    "        return 0\n",
    "    return max(getdepth(clust.right), getdepth(clust.left)) + clust.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drawdendrogram(clust, labels, jpeg='clusters.jpg'):\n",
    "    h = getheight(clust) * 20\n",
    "    w = 1200\n",
    "    depth = getdepth(clust)\n",
    "    \n",
    "    scaling = float(w-150) / depth\n",
    "    \n",
    "    img = Image.new('RGB', (w,h), (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    draw.line((0, h/2, 10, h/2), fill=(255, 0, 0))\n",
    "    drawnode(draw, clust, 10, (h/2), scaling, labels)\n",
    "    img.save(jpeg, 'JPEG')\n",
    "    \n",
    "def drawnode(draw, clust, x, y, scaling, labels):\n",
    "    if clust.id >= 0:\n",
    "        draw.text((x+5, y-7), labels[clust.id], (0,0,0))\n",
    "        return\n",
    "    \n",
    "    h1 = getheight(clust.left) * 20\n",
    "    h2 = getheight(clust.right) * 20\n",
    "    top = y - (h1+h2)/2\n",
    "    bottom = y + (h1+h2)/2\n",
    "    \n",
    "    ll = clust.distance * scaling\n",
    "    \n",
    "    draw.line((x, top+h1/2, x, bottom-h2/2), fill=(255,0,0))\n",
    "    draw.line((x, top+h1/2, x+11, top+h1/2), fill=(255,0,0))\n",
    "    draw.line((x, bottom-h2/2, x+11, bottom-h2/2), fill=(255,0,0))\n",
    "    \n",
    "    drawnode(draw, clust.left, x+11, top+h1/2, scaling, labels)\n",
    "    drawnode(draw, clust.right, x+11, bottom-h2/2, scaling, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drawdendrogram(clust, blognames, jpeg='blogclust.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Column Clustering\n",
    "\n",
    "def rotatematrix(data):\n",
    "    newdata = []\n",
    "    for i in range(len(data[0])):\n",
    "        newrow = [data[j][i] for j in range(len(data))]\n",
    "        newdata.append(newrow)\n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdata = rotatematrix(data)\n",
    "wordclust = hcluster(rdata)\n",
    "drawdendrogram(wordclust, labels=words, jpeg='wordclust.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kcluster(rows, distance=pearson, k=4):\n",
    "    ranges = [(min([row[i] for row in rows]), max([row[i] for row in rows]))\n",
    "                 for i in range(len(rows[0]))]\n",
    "    \n",
    "    # k random center\n",
    "    clusters = [[random.random() * (ranges[i][1]-ranges[i][0]) + ranges[i][0]\n",
    "                for i in range(len(rows[0]))] for j in range(k)]\n",
    "    \n",
    "    lastmatches = None\n",
    "    for t in range(100):\n",
    "        print('Iteration {0}'.format(t))\n",
    "        bestmatches = [[] for i in range(k)]\n",
    "        \n",
    "        for j in range(len(rows)):\n",
    "            row = rows[j]\n",
    "            bestmatch = 0\n",
    "            for i in range(k):\n",
    "                d = distance(clusters[i], row)\n",
    "                if d < distance(clusters[bestmatch], row):\n",
    "                    bestmatch = i\n",
    "            bestmatches[bestmatch].append(j)\n",
    "        \n",
    "        if bestmatches == lastmatches:\n",
    "            break\n",
    "        lastmatches = bestmatches\n",
    "        \n",
    "        # move the center to the group average point\n",
    "        for i in range(k):\n",
    "            avgs = [0.0] * len(rows[0])\n",
    "            if len(bestmatches[i]) > 0:\n",
    "                for rowid in bestmatches[i]:\n",
    "                    for m in range(len(avgs)):\n",
    "                        avgs[m] += rows[rowid][m]\n",
    "                for j in range(len(avgs)):\n",
    "                    avgs[j] /= len(bestmatches[i])\n",
    "                clusters[i] = avgs\n",
    "    \n",
    "    return bestmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n",
      "Iteration 30\n",
      "Iteration 31\n",
      "Iteration 32\n",
      "Iteration 33\n",
      "Iteration 34\n",
      "Iteration 35\n",
      "Iteration 36\n",
      "Iteration 37\n",
      "Iteration 38\n",
      "Iteration 39\n",
      "Iteration 40\n",
      "Iteration 41\n",
      "Iteration 42\n",
      "Iteration 43\n",
      "Iteration 44\n",
      "Iteration 45\n",
      "Iteration 46\n",
      "Iteration 47\n",
      "Iteration 48\n",
      "Iteration 49\n",
      "Iteration 50\n",
      "Iteration 51\n",
      "Iteration 52\n",
      "Iteration 53\n",
      "Iteration 54\n",
      "Iteration 55\n",
      "Iteration 56\n",
      "Iteration 57\n",
      "Iteration 58\n",
      "Iteration 59\n",
      "Iteration 60\n",
      "Iteration 61\n",
      "Iteration 62\n",
      "Iteration 63\n",
      "Iteration 64\n",
      "Iteration 65\n",
      "Iteration 66\n",
      "Iteration 67\n",
      "Iteration 68\n",
      "Iteration 69\n",
      "Iteration 70\n",
      "Iteration 71\n",
      "Iteration 72\n",
      "Iteration 73\n",
      "Iteration 74\n",
      "Iteration 75\n",
      "Iteration 76\n",
      "Iteration 77\n",
      "Iteration 78\n",
      "Iteration 79\n",
      "Iteration 80\n",
      "Iteration 81\n",
      "Iteration 82\n",
      "Iteration 83\n",
      "Iteration 84\n",
      "Iteration 85\n",
      "Iteration 86\n",
      "Iteration 87\n",
      "Iteration 88\n",
      "Iteration 89\n",
      "Iteration 90\n",
      "Iteration 91\n",
      "Iteration 92\n",
      "Iteration 93\n",
      "Iteration 94\n",
      "Iteration 95\n",
      "Iteration 96\n",
      "Iteration 97\n",
      "Iteration 98\n",
      "Iteration 99\n"
     ]
    }
   ],
   "source": [
    "kclust = kcluster(data, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[blognames[r] for r in kclust[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tanimoto coefficient\n",
    "\n",
    "def tanimoto(v1, v2):\n",
    "    \n",
    "    c1 = sum(v1)\n",
    "    c2 = sum(v2)\n",
    "    shr = sum([i1 and i2 for i1, i2 in zip(v1, v2)])\n",
    "    \n",
    "    return 1 - (shr/(c1 + c2 - shr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wants, people, data = readfile('zebo.txt')\n",
    "wclust = hcluster(data, distance=tanimoto)\n",
    "drawdendrogram(wclust, wants, jpeg='wants.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scaledown(data, distance=pearson, rate=0.000001):\n",
    "    n = len(data)\n",
    "    \n",
    "    # real distance\n",
    "    realdist = [[distance(data[i], data[j]) for j in range(n)]\n",
    "               for i in range(n)]\n",
    "    outersum = 0.0\n",
    "    \n",
    "    # random initial loc\n",
    "    loc = [[random.random(), random.random()] for i in range(n)]\n",
    "    fakedist = [[0.0 for j in range(n)] for i in range(n)]\n",
    "    \n",
    "    lasterror = None\n",
    "    for m in range(1000):\n",
    "#         fakedist = [[distance(loc[i], loc[j]) for j in range(n)]\n",
    "#                    for i in range(n)]\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                fakedist[i][j] = sqrt(sum([pow(loc[i][x] - loc[j][x], 2)\n",
    "                                          for x in range(len(loc[i]))]))\n",
    "        \n",
    "        # move the loc\n",
    "        grad = [[0.0, 0.0] for i in range(n)]\n",
    "        \n",
    "        totalerror = 0\n",
    "        for k in range(n):\n",
    "            for j in range(n):\n",
    "                if j == k: continue\n",
    "                errorterm = (fakedist[j][k] - realdist[j][k]) / realdist[j][k]\n",
    "                \n",
    "                grad[k][0] += ((loc[k][0] - loc[j][0])/fakedist[j][k]) * errorterm\n",
    "                grad[k][1] += ((loc[k][1] - loc[j][1])/fakedist[j][k]) * errorterm\n",
    "#                 grad[k][0] += (loc[k][0] - loc[j][0]) * errorterm\n",
    "#                 grad[k][1] += (loc[k][1] - loc[j][1]) * errorterm\n",
    "                \n",
    "                totalerror += abs(errorterm)\n",
    "        print(totalerror)\n",
    "        \n",
    "        if lasterror and lasterror < totalerror: break\n",
    "        lasterror = totalerror\n",
    "        \n",
    "        # TODO: how to use matric to simplify\n",
    "        for k in range(n):\n",
    "            loc[k][0] -= rate * grad[k][0]\n",
    "            loc[k][1] -= rate * grad[k][1]\n",
    "    \n",
    "    return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw2d(data, labels, jpeg='mds2d.jpg'):\n",
    "    img = Image.new('RGB', (2000,2000), (255,255,255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for i in range(len(data)):\n",
    "        x = (data[i][0] + 0.5) * 1000\n",
    "        y = (data[i][1] + 0.5) * 1000\n",
    "        draw.text((x,y), labels[i], (0,0,0))\n",
    "    img.save(jpeg, 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7257.270927227234\n",
      "7257.9170936138125\n"
     ]
    }
   ],
   "source": [
    "blognames, words, data = readfile('blogdata.txt')\n",
    "coords = scaledown(data)\n",
    "draw2d(coords, blognames, jpeg='blogs2d.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.452014601910929\n",
      "4.4180703942826804\n",
      "4.383644441201949\n",
      "4.34874059357193\n",
      "4.313363149833152\n",
      "4.2775168584904515\n",
      "4.241206919700445\n",
      "4.204438985894383\n",
      "4.167219161415186\n",
      "4.129554001151564\n",
      "4.0914505081565675\n",
      "4.052916130242429\n",
      "4.0139587555483125\n",
      "3.974586707082471\n",
      "3.934808736245241\n",
      "3.894634015344341\n",
      "3.854072129118964\n",
      "3.813133065294149\n",
      "3.7718272041918692\n",
      "3.7301653074300374\n",
      "3.688158505745364\n",
      "3.6458182859803765\n",
      "3.6031564772791964\n",
      "3.5689799876889663\n",
      "3.5681127474588648\n",
      "3.5674791230927005\n",
      "3.5670828957518346\n",
      "3.566927448080561\n",
      "3.5670157463540937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.7674815956855695, 0.7133326500178413],\n",
       " [-0.05655309807705622, 0.5758170530068707],\n",
       " [1.1904342383411575, 0.3517107416774642]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledown([[2,1], [1,2], [3,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
